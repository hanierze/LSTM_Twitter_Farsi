{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tikHxLTZhcp"
   },
   "source": [
    "# LSTM for twitter farsi datase\n",
    "### by maryam babaei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glN45G6iZzHp"
   },
   "source": [
    "### read file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUKx2SLD7SIc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = []\n",
    "f = open(\"X.txt\", \"r\")\n",
    "df = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65i3KIo1aeJY"
   },
   "source": [
    "### check dataset for cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "iYVIhj64bM_m",
    "outputId": "76c6e0ad-34ac-495a-8a62-372d1e0d96b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "به نظر شما وقتی جرمی هانت وزیر امورخارجه بریتانیا برای آزادی نازنین زاغری اینقدر تقلا و پروپاگاندا می کند در حال ارسا\n",
      "\n",
      "بنر تصاویر دیکتاتور توسط کانونهای شورشی به آتش کشیده شد مجاهدین\n",
      "\n",
      "مشهد به واسطه وجود حرم مطهر امام رضا ع و تولیتش پتانسیل کم نظیری دارد که بعد از تهران این شهر را به دومین شهر مستعد در شب\n",
      "\n",
      "عاخه تو مغزشون کردن هرکسی که مثلا از نظام انتقاد کنه یا خلافش حرفی بزنه با سعودی و آمریکاست همینقدر پرت و متعصبن\n",
      "\n",
      "ولي پولشو ميداد راضي تر بودم\n",
      "\n",
      "بونگ بونگ بونگ بونگ بونگ بونگ بونگ بونگ بونگ بونگ بونگ بونگ\n",
      "\n",
      "از ساعت به بعد هم اساسی فرو میکنه\n",
      "\n",
      "ولی خودمونیم هیچی اون قر ریزای همراه آهنگ که افقی بصورت خوابیده توی تخت اتفاق میفته نمی شه\n",
      "\n",
      "پیام شادباش نوروزی شاهزاده رضا پهلوی\n",
      "\n",
      "امروز به شیرآباد زاهدان رفتمبعد از بازدید مدرسه به خانه یکی از دانش آموزانی رفتم که بعد از روشن کردن چراغ دچار آتش سوزی شده بود و برای خاموش کردن خودش دوان دوان به سمت گودال آب فاضلاب کوچه رفته بود و در نهایت فوت شده بوداین اتفاق را هیچ رسانه ای منتشر نکرده\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "for row in islice(df, 10):\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXgnaimy7YoD"
   },
   "outputs": [],
   "source": [
    "traindata=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytKpQMGybD6A"
   },
   "source": [
    "### remove other colunms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYXzUIq6buKh"
   },
   "source": [
    "### use re library to clean data and keep persian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gq_vMFGVbrQU"
   },
   "outputs": [],
   "source": [
    "#!pip install hazm\n",
    "#from __future__ import unicode_literals\n",
    "#from hazm import *\n",
    "#normalizer = Normalizer()\n",
    "#for t in range (len(traindata)):\n",
    "#    traindata[t]=normalizer.normalize(traindata[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgKtVOYEbx9h"
   },
   "outputs": [],
   "source": [
    "#from hazm import word_tokenize, Normalizer\n",
    "import re\n",
    "reviews_ints = []\n",
    "for t in range (len(traindata)):\n",
    "    text = traindata[t]\n",
    "    #text = text.replace('\\u200c',' ')\n",
    "    #text = text.replace('..',' ')\n",
    "    #text = text.replace('RT',' ')\n",
    "    text = re.sub(r'[^آ-ی۰-۹ ]', ' ', text)\n",
    "    text = text.strip()\n",
    "    traindata[t]=text\n",
    "    #normalized_text = normalizer.normalize(text)\n",
    "    #tokens = word_tokenize(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "e287lI58b03H",
    "outputId": "bec686bc-e60e-4939-a35e-d5bca7a72cc7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic": {
       "type": "string"
      },
      "text/plain": [
       "'هیچکس نخواهد فهمیدکه در زندگی هر آدمی یکنفر هست که دوست داشتنی ترین پنهان دنیاس تقدیم به همه اونایی که دوست داشتنی پ'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg14hSzzcAH_"
   },
   "source": [
    "### split words and add them to a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "US_07lw2b_Rn"
   },
   "outputs": [],
   "source": [
    "Words1=[]\n",
    "for i in range (0,len(traindata)):\n",
    "    Words1.append(traindata[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "aZjTTtwscHeG",
    "outputId": "2b7cc744-50a8-44da-b405-267b64abae92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['هیچکس',\n",
       " 'نخواهد',\n",
       " 'فهمیدکه',\n",
       " 'در',\n",
       " 'زندگی',\n",
       " 'هر',\n",
       " 'آدمی',\n",
       " 'یکنفر',\n",
       " 'هست',\n",
       " 'که',\n",
       " 'دوست',\n",
       " 'داشتنی',\n",
       " 'ترین',\n",
       " 'پنهان',\n",
       " 'دنیاس',\n",
       " 'تقدیم',\n",
       " 'به',\n",
       " 'همه',\n",
       " 'اونایی',\n",
       " 'که',\n",
       " 'دوست',\n",
       " 'داشتنی',\n",
       " 'پ']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(Words1)\n",
    "Words1[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsppT22QcVlY"
   },
   "outputs": [],
   "source": [
    "#!pip3 install parsivar\n",
    "#from parsivar import Tokenizer\n",
    "#Words=[]\n",
    "#for i in range (0,len(traindata)):\n",
    "#  my_normalizer = Normalizer()\n",
    "#  my_tokenizer = Tokenizer()\n",
    "#  Words.append(my_tokenizer.tokenize_words(my_normalizer.normalize(traindata[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mU-ayrz3cgt-"
   },
   "outputs": [],
   "source": [
    "data=set()\n",
    "for i in range(0,len(Words1)):\n",
    "  for j in range(0,len(Words1[i])):\n",
    "    data.add(Words1[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DVjveQBzcoDX",
    "outputId": "dc39694e-c126-4810-e30e-30550d682af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103390"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"اونا\" in data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvbD6iB5cZIZ"
   },
   "source": [
    "### Tokenize and encoded the words and prepare the dataset into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hI7aangf71sX"
   },
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H8gvjRcycqh_",
    "outputId": "d64aece2-a288-48d5-eec2-86083517e9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263393"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(1000)\n",
    "tokenizer.fit_on_texts(Words1)\n",
    "def partitionlines(lines,maxlen):\n",
    "    data_x=[]\n",
    "    data_y=[]\n",
    "    for line in lines:\n",
    "        words = line\n",
    "        for i in range (0,maxlen):\n",
    "            if i >=len(words)-1:\n",
    "                break\n",
    "            data_x.append(words[0:i+1])\n",
    "            data_y.append(words[i+1])\n",
    "        for i in range (maxlen,len(words)):\n",
    "            if i >=len(words)-1:\n",
    "                break\n",
    "            data_x.append(words[i-maxlen+1:i+1])\n",
    "            data_y.append(words[i+1])\n",
    "    return data_x,data_y\n",
    "\n",
    "maxlen=5\n",
    "lines=tokenizer.texts_to_sequences(Words1)\n",
    "data_x,data_y = partitionlines(lines,maxlen)\n",
    "len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SgMSMJuJdEb0",
    "outputId": "469ae2cf-58e1-46ac-b75f-399f8869f06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [2]\n",
      "y1: 149\n",
      "x2: [2, 149]\n",
      "y2: 37\n",
      "x3: [2, 149, 37]\n",
      "y3: 62\n"
     ]
    }
   ],
   "source": [
    "print('x1:',data_x[0])\n",
    "print('y1:',data_y[0])\n",
    "print('x2:',data_x[1])\n",
    "print('y2:',data_y[1])\n",
    "print('x3:',data_x[2])\n",
    "print('y3:',data_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bnPVsr4pc9c8",
    "outputId": "97dbec2a-d90b-442e-e8fb-9d2a9b20053b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['فارسی']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXLE5NIwdNuL"
   },
   "source": [
    "### Convert a class vector y to binary class matrix and shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "khLx1IXodpcW",
    "outputId": "6e7e684c-4aa1-47d2-c75d-65c31be789d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263393, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(data_x,maxlen=maxlen)\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(data_y)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzAr6wT7driN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.max(y_train,axis=1)\n",
    "indices = np.arange(0,len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train_shuffled = x_train[indices]\n",
    "y_train_shuffled= y_train[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LF_l_gTgdgd5"
   },
   "source": [
    "### separate data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEyHlYCwdvcL"
   },
   "outputs": [],
   "source": [
    "x_test = x_train[-1000:]\n",
    "x_train_partial = x_train[:-1000]\n",
    "\n",
    "y_test = y_train[-1000:]\n",
    "y_train_partial = y_train[:-1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUAxECKOdr0J"
   },
   "source": [
    "### make a model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcAowDiqdxiq"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def getmodel():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(1000, 128))\n",
    "    model.add(layers.LSTM(1024))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer= 'adam',loss=losses.categorical_crossentropy,metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "fkgRdnrdd1Xd",
    "outputId": "bd3cfd62-7bea-47d7-aa72-f0837ae17af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         128000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              4722688   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1025000   \n",
      "=================================================================\n",
      "Total params: 5,875,688\n",
      "Trainable params: 5,875,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1009914 samples, validate on 252479 samples\n",
      "Epoch 1/15\n",
      "1009914/1009914 [==============================] - 242s 240us/step - loss: 4.8846 - categorical_accuracy: 0.1764 - val_loss: 4.5815 - val_categorical_accuracy: 0.2242\n",
      "Epoch 2/15\n",
      "1009914/1009914 [==============================] - 241s 239us/step - loss: 3.9197 - categorical_accuracy: 0.3208 - val_loss: 4.3493 - val_categorical_accuracy: 0.2785\n",
      "Epoch 3/15\n",
      "1009914/1009914 [==============================] - 241s 239us/step - loss: 3.4034 - categorical_accuracy: 0.3923 - val_loss: 4.3326 - val_categorical_accuracy: 0.3009\n",
      "Epoch 4/15\n",
      "1009914/1009914 [==============================] - 239s 236us/step - loss: 3.0618 - categorical_accuracy: 0.4374 - val_loss: 4.3778 - val_categorical_accuracy: 0.3128\n",
      "Epoch 5/15\n",
      "1009914/1009914 [==============================] - 239s 236us/step - loss: 2.8101 - categorical_accuracy: 0.4696 - val_loss: 4.4667 - val_categorical_accuracy: 0.3197\n",
      "Epoch 6/15\n",
      "1009914/1009914 [==============================] - 238s 235us/step - loss: 2.6108 - categorical_accuracy: 0.4947 - val_loss: 4.5724 - val_categorical_accuracy: 0.3226\n",
      "Epoch 7/15\n",
      "1009914/1009914 [==============================] - 237s 234us/step - loss: 2.4537 - categorical_accuracy: 0.5156 - val_loss: 4.6757 - val_categorical_accuracy: 0.3271\n",
      "Epoch 8/15\n",
      "1009914/1009914 [==============================] - 236s 233us/step - loss: 2.3249 - categorical_accuracy: 0.5319 - val_loss: 4.7789 - val_categorical_accuracy: 0.3283\n",
      "Epoch 9/15\n",
      "1009914/1009914 [==============================] - 235s 233us/step - loss: 2.2175 - categorical_accuracy: 0.5465 - val_loss: 4.8851 - val_categorical_accuracy: 0.3292\n",
      "Epoch 10/15\n",
      "1009914/1009914 [==============================] - 233s 231us/step - loss: 2.1343 - categorical_accuracy: 0.5571 - val_loss: 4.9784 - val_categorical_accuracy: 0.3299\n",
      "Epoch 11/15\n",
      "1009914/1009914 [==============================] - 232s 229us/step - loss: 2.0623 - categorical_accuracy: 0.5668 - val_loss: 5.0719 - val_categorical_accuracy: 0.3323\n",
      "Epoch 12/15\n",
      "1009914/1009914 [==============================] - 232s 230us/step - loss: 2.0028 - categorical_accuracy: 0.5746 - val_loss: 5.1663 - val_categorical_accuracy: 0.3302\n",
      "Epoch 13/15\n",
      "1009914/1009914 [==============================] - 233s 231us/step - loss: 1.9531 - categorical_accuracy: 0.5811 - val_loss: 5.2310 - val_categorical_accuracy: 0.3317\n",
      "Epoch 14/15\n",
      "1009914/1009914 [==============================] - 233s 231us/step - loss: 1.9142 - categorical_accuracy: 0.5861 - val_loss: 5.3182 - val_categorical_accuracy: 0.3305\n",
      "Epoch 15/15\n",
      "1009914/1009914 [==============================] - 233s 231us/step - loss: 1.8798 - categorical_accuracy: 0.5905 - val_loss: 5.3694 - val_categorical_accuracy: 0.3308\n"
     ]
    }
   ],
   "source": [
    "model = getmodel()\n",
    "history = model.fit(x_train_partial,\n",
    "          y_train_partial,\n",
    "          epochs=15,\n",
    "          batch_size=80,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECqU-ZoHegbz"
   },
   "source": [
    "### test the model and train model with new data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeBRfskZd7Lv"
   },
   "outputs": [],
   "source": [
    " reverse_word_index = {index:word for word,index in tokenizer.word_index.items()}\n",
    "def wordfromonehot(onehot):\n",
    "    index = np.argmax(onehot)\n",
    "    return reverse_word_index[index]\n",
    "def predict(input_line, count ):\n",
    "    input_line_clean = input_line.strip().lower()\n",
    "    predicted_words = []\n",
    "    input_x= tokenizer.texts_to_sequences([input_line_clean])[0]\n",
    "    for _ in range(0,count):\n",
    "        model_input_x = sequence.pad_sequences([input_x],maxlen=maxlen)\n",
    "        output_y = model.predict(model_input_x)\n",
    "        input_x.append(np.argmax(output_y))\n",
    "        predicted_words.append((wordfromonehot(output_y), np.max(output_y)))\n",
    "    print (\"input = {0}, predicted = {1}\".format(input_line,predicted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XZhuqFWSsv8W",
    "outputId": "9dd2b8f2-5480-4aa5-80c9-9a596f333860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = جمهوری اسلامی, predicted = [('ایران', 0.8559116), ('در', 0.38355848), ('حال', 0.9022368), ('شدن', 0.95495117), ('به', 0.7973923)]\n"
     ]
    }
   ],
   "source": [
    "predict(\"جمهوری اسلامی\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "TI-Na7uJxytk",
    "outputId": "99456d74-bf5b-4324-c3ee-e1b575abe98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = زندان اوین, predicted = [('شکنجه', 0.8871477), ('شدیم', 0.95050395), ('شدیم', 0.99152124), ('شدیم', 0.9983607), ('رو', 0.97632277), ('رو', 0.79682314), ('و', 0.8264079), ('رو', 0.97657245), ('روشن', 0.14543259), ('کنم', 0.25588527)]\n"
     ]
    }
   ],
   "source": [
    "predict(\"زندان اوین\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "accy-39k3B4G",
    "outputId": "d7d554d5-485c-4d74-b5a0-7de83fb65440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = مریم, predicted = [('رجوی', 0.88469017), ('اعتراف', 0.38268185), ('رئیس', 0.91869354), ('اتمی', 0.9999603), ('آخوندها', 0.99874735), ('به', 0.9991221), ('فریبکاری', 0.9996784), ('در', 0.99840754), ('برجام', 0.99951434), ('دروغ', 0.99901485)]\n"
     ]
    }
   ],
   "source": [
    "predict(\"مریم\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Lofdvw60ja4H",
    "outputId": "47252654-f82e-473a-9b79-051e90318be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 88us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.965951259613037, 0.2669999897480011]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "JmolxP1ykeOL",
    "outputId": "539b05f7-52ab-4e58-c96b-c08dfa1a2199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = از بیش, predicted = [('از', 0.9146901), ('کشور', 0.67938197), ('در', 0.9100147), ('داریم', 0.96381354)]\n",
      "input = از بیش از, predicted = [('کشور', 0.67938197), ('در', 0.9100147), ('داریم', 0.96381354), ('و', 0.32383752)]\n",
      "input = از بیش از کشور, predicted = [('در', 0.9100147), ('داریم', 0.96381354), ('و', 0.32383752), ('بر', 0.62237144)]\n",
      "input = از بیش از کشور در, predicted = [('داریم', 0.96381354), ('و', 0.32383752), ('بر', 0.62237144), ('قرار', 0.481333)]\n",
      "input = از, predicted = [('آب', 0.061724823), ('تا', 0.95944643), ('اینترنت', 0.98444563), ('از', 0.999966)]\n",
      "input = از برای, predicted = [('کردن', 0.27752125), ('ولی', 0.22692168), ('صحبت', 0.268811), ('داشتن', 0.3691872)]\n",
      "input = از برای استفاده, predicted = [('از', 0.49750358), ('در', 0.23249424), ('برابر', 0.68512595), ('ونزوئلا', 0.4377253)]\n",
      "input = از برای استفاده وقتی, predicted = [('که', 0.27600476), ('تمام', 0.40722522), ('شده', 0.5377303), ('استفاده', 0.19228534)]\n",
      "input = از برای استفاده وقتی دیگه, predicted = [('هیچ', 0.19984049), ('وقت', 0.22030741), ('نسبت', 0.43292233), ('به', 0.4180262)]\n",
      "input = اخه, predicted = [('چقدر', 0.18465437), ('کنیم', 0.70650214), ('میگن', 0.92206913), ('که', 0.9683194)]\n"
     ]
    }
   ],
   "source": [
    "x_test_words = []\n",
    "for seq in x_test:\n",
    "    x_test_words.append(' '.join([reverse_word_index[num] for num in seq if num != 0 ]))\n",
    "pred_test = model.predict(x_test)\n",
    "\n",
    "y_test_pred_words = [wordfromonehot(pred) for pred in pred_test]\n",
    "y_act = [wordfromonehot(y) for y in y_test]\n",
    "for i in range(0,10):\n",
    "    #print('input = {0}, pred  = {1}, actual = {2}'.format(x_test_words[i],y_test_pred_words[i],y_act[i]))\n",
    "    predict(x_test_words[i],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xhLbHnKkfkW"
   },
   "outputs": [],
   "source": [
    "model.save('partly_trained.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7i6iZlJ_mikI",
    "outputId": "7933b7d3-b6f4-44f2-dea3-0876a8374558"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('partly_trained.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3TwitterFarsi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
